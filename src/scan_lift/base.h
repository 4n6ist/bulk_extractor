#ifndef _SCAN_LIFT_BASE_H_
#define _SCAN_LIFT_BASE_H_

#include "myheader.h"
#include "classifier_config.h"

/**
 * The Document is a collection of features, labels, predicated labels, and scores
 * that were generated by n-gram extraction.
 */
class Document {
public: 
    Document() : docid(0),Features(),labels(),predicted_labels(),label_scores() {}
    virtual ~Document() {}

    int docid;

    vector< pair<int,double> > Features;
    vector<  int > labels;
    vector<  int > predicted_labels;
    vector< pair<  int , double > > label_scores;

    // Functions 

    virtual void normalize_predicted_scores();
    virtual void normalize_to_unit_length();
    virtual void print( FILE *f ) const;
    //virtual void print_arff( FILE *f ) const;
    virtual void print_predicted( FILE *f ) const;
    virtual bool parse_from_string( const char *str );

    typedef std::vector< pair<int,double> >::iterator iterator;
    typedef std::vector< pair<int,double> >::const_iterator const_iterator;

    iterator begin()		 { return Features.begin(); }
    iterator end()		 { return Features.end(); }

    const_iterator end() const   { return Features.end(); }
    const_iterator begin() const { return Features.begin(); }
};


/**
 * DataBundle is a class that bundles together:
 * labels
 * number of features
 * name of the dataset 
 * 1 or more Documents
 */

struct DataBundle {

    DataBundle( bool is_training_  , string dataset_name_ ):labels(),label_counts(),
							    nfeatures(0),is_multilabel(0),
							    is_training(is_training_),
							    dataset_name(dataset_name_),docs(){
    };
    virtual ~DataBundle(){}

    void normalize_to_unit_length();
    void normalize_predicted_scores( );
    void parse_from_file( const string& filename );
    void parse_from_file( FILE * );
    void save_scores( const string& filename ); // Not const because it can sort the label scores
    void save_dec( const string& filename ); // not const because it can sort the labels
    void push_document( const Document& d );

    void print( FILE *f ) const;
    //void print_arff( FILE *f ) const;
    void print_predicted( FILE *f ) const;

    vector<int> classes( ) const                       { return vector<int> ( labels.begin() , labels.end() ); }        
    vector< pair<int,int> > class_counts() const       { return vector< pair<int,int> > ( label_counts.begin() , label_counts.end() ); }
    string get_dataset_name() const                    { return dataset_name; }
    void set_dataset_name( const string& name)         { dataset_name = name; }
    int get_nfeatures() const                          { return nfeatures; } 
    int size() const                                   { return docs.size(); }
  
    typedef std::vector< Document >::iterator iterator;
    typedef std::vector< Document >::const_iterator const_iterator;

    iterator begin()                                     { return docs.begin(); }
    iterator end()                                       { return docs.end(); }

    const_iterator end() const                           { return docs.end(); }
    const_iterator begin() const                         { return docs.begin(); }

    set<int>     labels;
    map<int,int> label_counts;
    int nfeatures, is_multilabel, is_training;
    string dataset_name;
    vector< Document > docs;

#if 0
    static void print_arff_label_file( const DataBundle& training , const DataBundle& testing , FILE *f ) {
	assert( f );
	vector<int> c1 = training.classes( ), c2 = testing.classes( );
	set<int> L;
	EACH(it,c1) L.insert( *it ); EACH(it,c2) L.insert( *it );
	fprintf(f,"<\?xml version=\"1.0\" encoding=\"utf-8\"\?>\n<labels xmlns=\"http://mulan.sourceforge.net/labels\">\n");
    
	EACH(it,L) fprintf( f , "<label name=\"Class%d\"></label>\n" , (*it) );
	fprintf( f , "</labels>\n");
    }
#endif
    static void normalize_to_zero_mean_unit_variance( DataBundle& training , DataBundle& testing ) {
	int nfeatures = training.get_nfeatures();
	double Mean[ nfeatures+1 ], std_dev[ nfeatures+1 ] , non_zero_count[ nfeatures + 1 ];
	REP(i,nfeatures+1) Mean[ i ] = std_dev[ i ] = non_zero_count[ i ] = 0;
	EACH(it,training) EACH(f,(*it)) Mean[ f->first ] += f->second , non_zero_count[ f->first ]++;
	REP(i,nfeatures+1) if( non_zero_count[i] > 1e-9 ) Mean[ i ] /= non_zero_count[ i ];
	EACH(it,training) EACH(f,(*it)) if( f->first <= nfeatures ) 
	    std_dev[ f->first ] += (Mean[ f->first ] - f->second) * (Mean[ f->first ] - f->second);
	REP(i,nfeatures+1) std_dev[ i ] = sqrt( std_dev[ i ] / ( non_zero_count[ i ] - 1 ) );
	/* Apply the means and std_dev to each bundle */
	EACH(it,training) EACH(f,(*it))
	    f->second =  std_dev[ f->first ] < EPS ? 0 : ( f->second - Mean[ f->first ] ) / std_dev[ f->first ];

	EACH(it,testing) EACH(f,(*it)) 
	    f->second =  std_dev[ f->first ] < EPS ? 0 : ( f->second - Mean[ f->first ] ) / std_dev[ f->first ];
    }

};


class BaseClassifier {
public:
    ClassifierConfig config; 
    inline BaseClassifier( const ClassifierConfig& _config ) : config( _config ) {}
    virtual void clear() = 0;
    virtual void re_train( DataBundle& ) = 0;
    virtual void classify( DataBundle& ) const = 0;
    virtual void classify_document( Document& ) const = 0;
    virtual vector< pair<int,double> >  get_classifier_score(const Document& d ) const = 0;
    virtual double get_classifier_score_for_class(const Document& d , int class_id ) const = 0;
    virtual ~BaseClassifier();
};

#endif
